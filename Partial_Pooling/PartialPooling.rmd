---
title: "Partial Pooling with Custom MCMC in Nimble"
author: "Arman Oganisian, twitter: @stablemarkets"
date: "7/4/2018"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, error = F, message = F)
```

## Motivation and Simulated Data

In this demo, I'll highlight some features of Nimble I like through a toy, simulated data example.

Consider a financial analysis of mergers, where we are interested in estimating the probability of a merger going through once merger proceedings have started. Mergers may fall apart or not be approved by the government, so not all initiated mergers are completed. We have historical data on the proportion of mergers that were completed by industry for five industries. Here are some observations to give you a sense of the data:

```{r}
set.seed(10)

# simulate data for industries A, B, C, ... , E
d_A<-rbinom(n = 10,  size = 1, prob =  .2)
d_B<-rbinom(n = 100, size = 1, prob = .3)
d_C<-rbinom(n = 10, size = 1, prob = .45)
d_D<-rbinom(n = 100, size = 1, prob = .7)
d_E<-rbinom(n = 10, size = 1, prob = .8)

# stack, combine data and industry indicators
complete <- c(d_A, d_B, d_C, d_D, d_E)
industry <- as.factor(c(rep('A', 10),  rep('B', 100), rep('C', 10),
              rep('D', 100), rep('E', 10) ))

# obtain the model matrix consisting of industry indicators and a constant.
# industry A is the reference for the other indicators.
mod_mat <- model.matrix(lm(complete ~ industry))

# data to feed into Nimble
d_list <- list(X = mod_mat, 
               complete = complete)

# constants to feed into Nimble
p <- ncol(mod_mat)
n <- nrow(mod_mat)

# glimpse of the data
data.frame(complete, industry)[105:120,]
```

## Unpooled and Fully Pooled Analyses

The observed merger completion rates by industry are:
```{r}
library(dplyr)
library(tidyr)

unpooled <- data.frame(complete,industry) %>%
  group_by(industry) %>%
  summarise(completion_rate=mean(complete), n_mergers_started=n())

unpooled
```

The overall completion rate, pooled across industries, is 
```{r}
pooled <- mean(complete)
pooled
```

As we will see, the Bayesian estimate will be a compromise between these two extreme approaches: computing completely separate industry-specific estimates versus computing a single pooled estimate.

## Bayesian Analysis Using Nimble
Nimble is a nice tool for a lot of reasons.

1. It provides BUGS/JAGS like syntax all within the R environment.
2. It has a C++ back-end, so it's fast. 
3. BUGS/JAGS can be a black-box. It's not always clear what sampler is being used for each parameter being monitored. I mostly like Nimble because it allows me to customize the sampler used for each parameter, giving me more lower-level control than I could get with other software.

Below, wrapped in the `nimbleCode()` function, is the code specifying the following Bayesian logistic model for the $i^{th}$ merger's completion status:

$$ complete_i \sim Ber(p_i)  $$
Where 
$$ logit(p_i) = \beta_1 + \beta_2\cdot B_i + \beta_3\cdot C_i + \beta_4\cdot C_i + \beta_5\cdot E_i $$
Above, $(B_i,\dots, E_i )$ are industry dummies, with industry A as the reference. To complete a Bayesian specification, I place independent mean-zero Gaussian priors on the coefficients: $\beta_j \sim N(0,sd=2)$, for $j\in\{1, \dots, 5 \}$.

Of interest is not the $\beta_j$ themselves, but rather the industry-specific propability of completion - which is a non-linear transformation of a linear combination of the $\beta_j$. Specifically, for industry A it is $p_A = expit(\beta_1)$. For industry B, it is $p_B = expit(\beta_1 + \beta_2)$. For industry C, it is $p_C = expit(\beta_1 + \beta_3)$, and so on.

```{r}
library(nimble)

code <- nimbleCode({
  
  # specifiy priors for each beta
  for(i in 1:p){
    beta[i] ~ dnorm(0, 2)
  }
  
  # specify likelihood
  logit(eta[1:n]) <- X[1:n,1:p] %*% beta[1:p]
  
  for(i in 1:n) {
    complete[i] ~ dbern(prob = eta[i] )  
  }
  
  # compute merger success rate by industry
  # this is what we actually want... the betas
  # are just nuisance/intermediate estimates.
  p_comp[1] <- expit(beta[1])
  p_comp[2] <- expit(beta[1] + beta[2])
  p_comp[3] <- expit(beta[1] + beta[3])
  p_comp[4] <- expit(beta[1] + beta[4])
  p_comp[5] <- expit(beta[1] + beta[5])
  
})

```

A few things to note above. First, the matrix multiplication facility in Nimble. The matrix $X$ is $n\times p$, containing one column for each $\beta_j$ and one row for each of the $n$ initiated mergers.

Second, we can compute transformations of the $\beta_j$ to get back to the desired probabilities `p_comp`$=[p_A, p_B, p_C, p_D, p_E ]'$. For Stan users, this is equivalent to something you would do in the "generated quantities" block.

The code below declares the Nimble model and customizes the MCMC algorithm

```{r}

merge_model <- nimbleModel(code=code, 
                           constants=list(p=p, n=n),
                           inits = list(beta=c(0,0,0,0,0)),
                           data=d_list)

spec <- configureMCMC(merge_model)
spec$addSampler(type = 'RW_block', target ='beta',
                control = list(adaptive = TRUE ))

spec$resetMonitors()
spec$addMonitors('p_comp')

```

Using `spec$addSampler()`, I specify a blocked metropolis-hastings algorithm for sampling from the posterior of the $\beta_j$ parameters using `type = 'RW_block'`. I specify an adaptive algorithm that will tune the variance of the proposal distribution with `adaptive = TRUE`. This is in fact the default, but I wanted to show that this level of customization is possible. In the `control` list, we can also provide a custom initial covariance matrix for the proposal distribution. This can speed up mixing when the covariates have very different scales.

Finally, since the `beta` vector in the Nimble model is not of interest, I configure the MCMC to only monitor `p_comp` - the vector of merger completion probabilities. This is done by first reseting parameters to be monitored with `spec$resetMonitors()` then adding the parameter vector we want to monitor `spec$addMonitors('p_comp')`.

Finally, the code below builds and compiles the model and custom MCMC. I take 5,000 posterior draws after a 5,000 burn-in and compute `p_comp` using the `runMCMC()` function.

```{r}
mcmc <- buildMCMC(spec)

compiled_model <- compileNimble(merge_model)
compiled_mcmc <- compileNimble(mcmc, project = merge_model)

# take 10,000 draws with 5,000 burn-in
posterior_draws <- runMCMC(compiled_mcmc, 
                           niter = 10000, nburnin = 5000, nchains = 1,
                           summary = TRUE)

# compute summary statistics of the posterior draws.
posterior_draws$summary

# store posterior means
partial_pool_p <- posterior_draws$summary[,'Mean']
```

## Comparing Results
The chart below compares the estimates from the unpooled (stratified), pooled, and Bayesian estimates.
```{r, fig.width=10, fig.height=7}
plot(partial_pool_p, pch=20, col='red', ylim=c(0,1), axes=F, 
     xlab='Industry', ylab='Probability of Merger Completion')
axis(1, at = 1:5, labels = paste0(unique(industry), " (n =",c(10,100,10,100,10),')' ) )
axis(2, at = seq(0,1,.2), labels= seq(0,1,.2) )

points(1:5, unpooled$completion_rate, pch=20, col='black')
abline(h=pooled, lty=2)

legend('bottomright', 
       legend = c('Pooled Estimate','Stratified Estimates', 'Bayesian Estimate'),
       lty = c(2,NA,NA), col=c('black','black','red'), pch=c(NA, 20,20), bty='n')
```

We can see that the prior on the betas in our Bayesian model induces a shrinkage on the industry-specific estimates. The industry-specific estimates are shrunk towards the overall, pooled completion probability indicated by the horizontal line. In this sense, the Bayesian estimates as "partially" pooled.

For industries with very few initiated mergers (e.g. industries A, C, and E), the shrinkage from the stratified estimates to the pooled is more extreme. We have few data points, so the prior is driving the estimate more heavily. For industries with more data (e.g. industries B and D), the shrinkage is less pronounced. We have more data, so it is more influential for these industry estimates. 

This partially pooled estimate is nice and intuitive. For industry A, it's unlikely that the true probability of merger completion is 0\%. It's likely that the probability is low and we just did not observe enough mergers. Nevertheless, we are able to borrow information from the other industries via shrinkage towards the pooled probability.

A tighter prior around $\beta_j$ would yield more shrinkage towards the pooled estimate, while a wider prior would reduce the degree of shrinkage.

## Session Information
```{r}
sessionInfo()
```

## References 
1. Gelman, Andrew, et al. Bayesian data analysis. Chapman and Hall/CRC, 1995.
2. McElreath, Richard, -. Statistical Rethinking: a Bayesian Course with Examples in R and Stan /Boca Raton: CRC Press Taylor & Francis Group, 2016. Print.
3. Perry de Valpine, Daniel Turek, Christopher Paciorek, Clifford Anderson-Bergman, Duncan Temple Lang and Rastislav Bodik (2017). Programming with models: writing statistical algorithms for general model structures with NIMBLE. \textit{Journal of Computational and Graphical Statistics}, 26(2): 403-413.

