---
title: "Bayesian Nonparametric Models in NIMBLE"
subtitle: "September 2018"
author: "Claudia Wehrhahn, Chris Paciorek, and the NIMBLE Development Team"
output:
  html_document:
    code_folding: show
bibliography: [refs.bib]
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE} 
library(methods) ## needed only when building documents outside of R
library(nimble)
library(TeachingDemos) ## for computing a empirical HPD pointwise credible band
```

***(CHRIS:  This version of the blog post is a bit long.  We might want to consider breaking it in two.)***

NIMBLE is a hierarchical modeling package that uses nearly the same language for model specification as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible -- you can add distributions and functions -- and also allowing customization of the algotithms used to estimate the parameters of the model.

Recently, we added support for Markov chain Monte Carlo inference of Bayesian nonparametric (BNP) mixture models to NIMBLE. In particular, starting with version 0.6-11, NIMBLE provides functionality for fitting models involving Dirichlet process priors using either the Chinese Restaurant Process (CRP), or a truncated stick-breaking (SB) representation of the Dirichlet process prior.

We will illustrate NIMBLE's BNP capabilities using two examples.  First, we will show how to use nonparametric mixture models with different kernels for density estimation.  Then, we take a parametric generalized linear mixed model and show how to switch it into a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.

## Basic density estimation using Dirichlet Process Mixture models

NIMBLE provides the machinery for nonparametric density estimation by means of Dirichlet process mixture (DPM) models.  For an independent and identically distributed sample $y_1, \ldots, y_n$, the model takes the form
$$
y_i \mid \theta_i \sim p(y_i \mid \theta_i), \quad\quad \theta_i \mid G \sim G, \quad\quad G \sim \mbox{DP}(\alpha, H), \quad\quad i=1,\ldots, n .
$$

The NIMBLE implementation of this model is flexible and allows for mixtures of arbitrary kernels $p(y_i \mid \theta)$, which can be either conjugate or non-conjugate to the (also arbitrary) base meassure $H$.  In the case of conjugate kernel / base meassure pairs, NIMBLE is able to detect the presence of the conjugacy and use it to improve the performance of the sampler.

To illustrate these capabilities, we consider the estimation of the probability density function of the waiting time between eruptions of the Old Faithfull volcano data set available in the base R distribution.  
```{r, normaldensity-bnp}
data(faithful)
```
The observations $y_1, \ldots, y_n$ correspond to the second column of the dataframe, and $n = 272$.


### Fitting a location-scale mixture of Gaussian distributions using the CRP representation

#### Model specification

We first consider a location-scale Dirichlet process mixture of normal distributionss fitted to the transformed data $y_i^{*} = \log (y_i)$:  
$$
y^{*}_i \mid \mu_i, \sigma^2_i \sim \mbox{N}(\mu_i, \sigma^2_i), \quad\quad (\mu_i, \sigma^2_i) \mid G \sim G, \quad\quad G \sim \mbox{DP}(\alpha, H), \quad\quad i=1,\ldots, n,
$$
where $H$ corresponds to a normal-inverse-gamma meassure.  This model can be interpreted as providing a Bayesian version of kernel density estimation for $y^{*}_i$ using Gaussian kernels and *adaptive bandwidths*.  In the original scale of the data, this translates into a adaptive log-Gaussian kernel density estimate.

Introducing auxiliary variables $\xi_1, \ldots, \xi_n$ that indicate which component of the mixture generates each observations, and integrating over the random meassure $G$, we obtain the CRP representation of the model:
$$
y_i^{*} \mid \{ \tilde{\mu}_k \}, \{ \tilde{\sigma}_k^{2} \} \sim \mbox{N}\left( \tilde{\mu}_{\xi_i}, \tilde{\sigma}^2_{\xi_i} \right), \quad\quad \xi \mid \alpha \sim \mbox{CRP}(\alpha), \quad\quad (\tilde{\mu}_k, \tilde{\sigma}_k^2) \mid H \sim H, \quad\quad i=1,\ldots, n ,
$$
where 
$$
p(\xi \mid \alpha) = \frac{\Gamma(\alpha)}{\Gamma(\alpha + n)} \alpha^{K(\xi)} \prod_k 
\Gamma\left(m_k(\xi)\right)  ,
$$
$K(\xi) \le n$ is the the number of unique values in the vector $\xi$, and $m_k(\xi)$ is the number of times the $k$-th unique value appears in $\xi$.  This specification makes it clear that each observation belongs to any of *at most* $n$ normally distributed clusters, and that the CRP distribution corresponds to the prior distribution on the partition structure. 

NIMBLE's BUGS-language specification of this model is given by
```{r, normaldensity-bnp-CRP}
code <- nimbleCode({
  for(i in 1:n) {
    y[i] ~ dnorm(mu[i], var = s2[i]) 
    mu[i] <- muTilde[xi[i]]
    s2[i] <- s2Tilde[xi[i]]
  }
  xi[1:n] ~ dCRP(alpha, size = n)
  for(i in 1:n) {
    muTilde[i] ~ dnorm(0, var = s2Tilde[i]) 
    s2Tilde[i] ~ dinvgamma(1, 1)
  }
  alpha ~ dgamma(1, 1)
})
```

Note that in the BUGS-language code the length of the parameter vectors 'muTilde' and 's2Tilde' has been set to $n$.  We do this because the current implementation of NIMBLE requires that the length of vector of parameters be set in advance and does not allow for their number to change between iterations.  Hence, if we are to ensure that the algorithm always works as intended we need to work with the worst case scenario, i.e., the case where there are as many components as observations.  While this ensures that the algorithm always works as intended, it is also somewhat inneficient, both in terms of memory requirements (when $n$ is large a large number of unoccupied components need to be maintained) and in terms of computational burden (a large number of parameters that are not required for posterior inference need to be updated at every iteration).

Note also that the value of $\alpha$ controls the number of components we expect a priori, with larger values of $\alpha$ corresponding to a larger number of components occupied by the data.  Hence, by assigning a prior to $\alpha$ we add flexibility to the model specification.  The particular choice of a Gamma prior allows NIMBLE to use a data-augmentation scheme to efficiently sample from the corresponding full conditional distribution.  Alternative prior specifications for $\alpha$ are possible, in which case the default sampler for this parameter is an adaptive random-walk Metropolis-Hastings algorithm.

#### Running the MCMC algorithm

The following code sets up the data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm.  Because the specification is in terms of a Chinese restaurant process, the default sampler selected by NIMBLE is a collapsed Gibbs sampler.
```{r, MCMC-normaldensity}
set.seed(1)
# Model Data
logFaithful <- log(faithful$waiting)
dataTransformed <- (logFaithful - mean(logFaithful)) / sd(logFaithful)
n <- length(dataTransformed)
data <- list(y = dataTransformed)
# Model Constants
consts = list(n = length(dataTransformed))
# Parameter initialization
inits <- list(xi = sample(1:10, size=n, replace=TRUE), 
              muTilde = rnorm(n, 0, sd = sqrt(10)), 
              s2Tilde = rinvgamma(n, 1, 1), 
              alpha = 1) 
# Model compilation
rmodel <- nimbleModel(code, data=data, inits=inits, constants = consts)        # the NIMBLE model
cmodel <- compileNimble(rmodel)                                                   # compiling the model in C
mConf <- configureMCMC(rmodel, monitors = c("xi", "muTilde", "s2Tilde", "alpha")) # configuration of the MCMC. 
mMCMC <- buildMCMC(mConf) 
cMCMC <- compileNimble(mMCMC, project = rmodel)                                   # compiling the MCMC 
samples <- runMCMC(cMCMC, niter = 2000, nburnin = 1000, setSeed = TRUE) # running the MCMC computation
```

We can extract the samples from the posterior distributions of the parameters and create trace plots, histograms, and any other summary of interest.  For example, for the concentration parameter $\alpha$ we have:
```{r, MCMC-normaldensity-somegraphs}
# plots: 
ts.plot(samples[ , "alpha"], xlab="iteration", ylab=expression(alpha)) # Trace plot of the posterior samples of the concentration parameter
hist(samples[ , "alpha"], xlab=expression(alpha), main="", ylab="Frequency") # Histogram of the posterior samples for the concentration parameter 
quantile(samples[ , "alpha"], c(0.5, 0.025, 0.975))
```

Under this model, the posterior predictive distribution for a new observation $\tilde{y}$, $p(\tilde{y} \mid y_1, \ldots, y_n)$ is the optimal density estimator (under square error loss).  Samples for this estimator can be easily computed from the samples generated by our MCMC:

```{r, MCMC-normaldensity-predictive}
alphaSamples <- samples[ , "alpha"] # posterior samples of the concentration parameter
muTildeSamples <- samples[ , grep('muTilde', colnames(samples))] # posterior samples of the mean cluster parameter
s2TildeSamples <- samples[ , grep('s2Tilde', colnames(samples))] # posterior samples of the variance cluster parameter
xiSamples <- samples [ , grep('xi', colnames(samples))] # posterior samples of the membership cluster parameter

grid <- seq(-2.5, 2.5, len = 200) # grid for new observation y.tilde

postPredictiveSamples <- matrix(0, ncol = length(grid), nrow = nrow(samples))
for(i in 1:nrow(samples)){
  k <- unique(xiSamples[i, ])
  kNew <- max(k) + 1
  mk <- c()
  li <- 1
  for(l in 1:length(k)) {
    mk[li] <- sum(xiSamples[i, ] == k[li])
    li <- li + 1
  }
  alpha <- alphaSamples[i]
  
  muK <-  muTildeSamples[i, k]
  s2K <-  s2TildeSamples[i, k]
  muKnew <-  muTildeSamples[i, kNew]
  s2Knew <-  s2TildeSamples[i, kNew]
  
  postPredictiveSamples[i, ] <- sapply(grid, 
                function(x)(sum(mk * dnorm(x, muK, sqrt(s2K))) + alpha * dnorm(x, muKnew, sqrt(s2Knew)) )/(alpha+n))
}

postPredicitveHat <- apply(postPredictiveSamples, 2, mean)

# plots:
hist(data$y, freq=FALSE, xlim=c(-2.5, 2.5), ylim=c(0,0.7), main = "", xlab = "Waiting times in log scale") # histogram of standardized log waiting time
lines(grid, apply(postPredictiveSamples, 2, mean), lwd=2, col='black') # pointwise estimate of the density
lines(grid, apply(postPredictiveSamples, 2, quantile, 0.025), lty=2, col='black') 
lines(grid, apply(postPredictiveSamples, 2, quantile, 0.975), lty=2, col='black') 
```

Recall, however, that this is the density estimate for the logarithm of the waiting time.  To obtain the density in the original scale we need to apply the appropriate transformation to the kernel.
```{r, MCMC-normaldensity-predictive-originalScale}
gridLogScale <- grid*sd(logFaithful) + mean(logFaithful) # grid in log scale
predictiveHatLogScale <- apply(postPredictiveSamples, 2, mean) / sd(logFaithful)
predictiveHatLogScaleCIinf <- apply(postPredictiveSamples, 2, quantile, 0.025) / sd(logFaithful)
predictiveHatLogScaleCIsup <- apply(postPredictiveSamples, 2, quantile, 0.975) / sd(logFaithful)

hist(faithful$waiting, freq=FALSE, xlim=c(40, 100), main = "", xlab = "Waiting times") # histogram of waiting time in original scale
lines(exp(gridLogScale), predictiveHatLogScale/exp(gridLogScale), lwd=2, col='black')
lines(exp(gridLogScale), predictiveHatLogScaleCIinf/exp(gridLogScale), lty=2, col='black')
lines(exp(gridLogScale), predictiveHatLogScaleCIsup/exp(gridLogScale), lty=2, col='black')
```


In either case, there is clear evidence that the data has two components for the waiting times.

#### Generating samples from the mixing distribution

While samples from the posterior distribution of linear functionals of the mixing distribution $G$ (such as the predictive distribution above) can be computed directly from the realizations of the collapsed sampler, inference for non-linear functionals of $G$ requires that we first generate samples from the mixing distribution.  In NIMBLE we can get posterior samples from the random measure $G$, using the getSamplesDPmeasure() function.  Note that, in order to get posterior samples from G, we need to monitor all the random variables involved in its computations, i.e., the membership variable, 'xi', the cluster parameters, 'muTilde' and 's2Tilde', and the concentration parameter, 'alpha'.

The following code generates posterior samples from the random measure $G$. The 'cMCMC' object includes the model and posterior samples from the parameters. When running the getSamplesDPmeasure() function, a message displays the truncation level of $G$, say $truncG$. The resulting object is a matriz having $(truncG * (p+1))$ colums, where $p$ is the dimension of the vector of parameters with distribution $G$ (in this example $p=2$).
```{r, G-normaldensity}
samplesG <- getSamplesDPmeasure(cMCMC) 
```

The following code computes posterior samples of $P(\tilde{y} > 70)$ using the posterior samples from the random measure $G$. Note that these samples are computed based on the transformed model and a value larger than $70$ corresponds to a value larger than $-0.06281407$ in the above defined grid, which is the 98th component.

```{r, G-normaldensity-probYtildeLarger70}
truncG <- 30 # truncation level for G, obtained from running the getSamplesDPmeasure(cMCMC) line

probY70 <- c() # posterior samples of P(y.tilde > 70)
for(i in 1:nrow(samples)){
  probY70[i] <- sum(samplesG[i, grep('weight', colnames(samplesG))] * pnorm(-0.06281407, 
                              samplesG[i, grep('muTilde', colnames(samplesG))], 
                              sqrt(samplesG[i, grep('s2Tilde', colnames(samplesG))]), 
                              lower.tail = FALSE)) 
}

# plots: 
hist(probY70,  xlab = "iteration", ylab = expression(P(tilde(y) > 70)), main = paste(expression(P(tilde(y) > 70) | data) , "=", round(mean(probY70),3) ) )
```

### Fitting a mixture of gamma distributions using the CRP representation

NIMBLE is not restricted to using Gaussian kernels in DPM models.  In the case of the Old Faithful data, an alternative to the mixture of Gaussian kernels in the logarithmic scale that we presented in the previous section is a (scale-and-shape) mixture of Gamma distributions in the *original* scale of the data.  

#### Model specification

In this case, the model takes the form
$$
y_i \mid \{ \tilde{\beta}_k \}, \{ \tilde{\lambda}_k \} \sim \mbox{Gam}\left( \tilde{\beta}_{\xi_i}, \tilde{\lambda}_{\xi_i} \right), \quad\quad \xi \mid \alpha \sim \mbox{CRP}(\alpha), \quad\quad (\tilde{\beta}_k, \tilde{\lambda}_k) \mid H \sim H ,
$$
where $H$ corresponds to the product of two independent Gamma distributions.  The following code provides the NIMBLE specification for the model:
```{r, gammadensity-bnp}
code <- nimbleCode({
  for(i in 1:n) {
    y[i] ~ dgamma(shape = beta[i], scale = lambda[i])
    beta[i] <- betaTilde[xi[i]]
    lambda[i] <- lambdaTilde[xi[i]]
  }
  xi[1:n] ~ dCRP(alpha, size = n)
  for(i in 1:50) { # only 50 cluster parameters
    betaTilde[i] ~ dgamma(shape = 71, scale = 2)
    lambdaTilde[i] ~ dgamma(shape = 2, scale = 2)
  }
  alpha ~ dgamma(1, 1)
})
```

Note that in this case the vectors 'betaTilde' and 'lambdaTilde' have length $50 \ll n = 272$.  This is done to reduce the computational and storage burdens associated with the sampling algorithm.  You could think about this approach as truncating the process, except that it can be thought of as an *exact* truncation.  Indeed, under the CRP representation, using parameter vectors with a length that is shorter than the number of observations in the sample will lead to a proper algorithm as long as the number of components instatiated by the sampler is strictly lower than the length of vectors for every iteration of the sampler.


#### Running the MCMC algorithm

The following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions.  Note that, when buiding the MCMC, a warning message about the number of cluster parameters is generated. This is because the 'size' of 'betaTilde' and 'lambdaTilde' is smaller than $n$.  Also, note that no error message is generated during execution, which indicates that the number of clusters required never exceeded the maximum of 50.

```{r, MCMC-normaldensity2}
data <- list(y = faithful$waiting)
set.seed(1)
inits <- list(xi = sample(1:10, size=n, replace=TRUE), 
              betaTilde = rgamma(50, shape = 71, scale = 2), 
              lambdaTilde = rgamma(50, shape = 2, scale = 2), 
              alpha = 1) 
rmodel <- nimbleModel(code, data=data, inits=inits, constants = consts)
cmodel <- compileNimble(rmodel)
mConf <- configureMCMC(rmodel, monitors = c("xi", "betaTilde", "lambdaTilde", "alpha"))
mMCMC <- buildMCMC(mConf)
cMCMC <- compileNimble(mMCMC, project = rmodel)
samples <- runMCMC(cMCMC, niter = 2000, nburnin = 1000, setSeed = TRUE)
```

In this case we use the posterior samples of the parameters to construct a trace plot and estimate the posterior distribution of $\alpha$:
```{r, results-CRPalpha2}
# plots: 
ts.plot(samples[ , 'alpha'], xlab="iteration", ylab=expression(alpha)) # Trace plot of the posterior samples of the concentration parameter
hist(samples[ , 'alpha'], xlab=expression(alpha), ylab="Frequency", main="") # Histogram of the posterior samples for the concentration parameter 
```


#### Generating samples from the mixing distribution

As before, we obtain samples from the posterior distribution of $G$ using the 'getSamplesDPmeasure' function. 

```{r, G-normaldensity2}
samplesG <- getSamplesDPmeasure(cMCMC) 
```

We use these samples to create an estimate of the density of the data along with a pointwise 95\% credible band:
```{r, results-normaldensity}
truncG <- 25
grid <- seq(40, 100, len=200)

postSampleDensity <- matrix(0, ncol = length(grid), nrow = nrow(samples))
for(iter in 1:nrow(samples)){
  postSampleDensity[iter, ] <- sapply(grid, function(x)sum(samplesG[iter, grep('weight', colnames(samplesG))] * dgamma(x, shape = samplesG[iter, grep('betaTilde', colnames(samplesG))], scale = samplesG[iter, grep('lambdaTilde', colnames(samplesG))]))) 
}


hist(data$y, freq=FALSE, xlim=c(40,100), main="", ylab="", xlab="Old Faithful data")
lines(grid, apply(postSampleDensity, 2, mean), lwd=2, col='black')
lines(grid, apply(postSampleDensity, 2, emp.hpd)[1, ], lwd=2, lty=2, col='grey')
lines(grid, apply(postSampleDensity, 2, emp.hpd)[2, ], lwd=2, lty=2, col='grey')
```

Again, we see that the density of the data is bimodal, and looks very similar to the one we obtained before.



### Fitting a DP mixture of Gammas using a stick-breaking representation

#### Model specification

An alternative representation of the Dirichlet process mixture uses the stick breaking representation of the random distribution $G$.  NIMBLE allows us to specify an approximation that involves a truncation the Dirichlet process to finite number of atoms of $L$.  The resulting model therefore reduces to a finite mixture with $L$ components a very particular prior on the weights of the mixture components.

Introducing auxiliary variables $z_1, \ldots, z_n$ that indicate which component generagted each observation, the corresponding model for the mixture of Gamma densities discussed in the previous section takes the form
$$
y_i \mid \{ {\beta}_k^{\star} \}, \{ {\lambda}_k^{\star} \}, z_i \sim \mbox{Gam}\left( {\beta}_{z_i}^{\star}, {\lambda}_{z_i}^{\star} \right), \quad\quad \boldsymbol{z} \mid \boldsymbol{w} \sim \mbox{Discrete}(\boldsymbol{w}), \quad\quad ({\beta}_k^{\star}, {\lambda}_k^{\star}) \mid H \sim H ,
$$
where $H$ is again to the product of two independent Gamma distributions, 
$$w_1=v_1, \quad\quad w_l=v_l\prod_{m<l}(1-v_m), \quad l=2, \ldots, L-1,\quad\quad w_L=\prod_{m<L}(1-v_m)$$
with $v_l \sim Beta(1, \alpha), l=1, \ldots, L-1$.  The following code provides the NIMBLE specification for the model:

```{r, gammadensity-bnp-SB}
code <- nimbleCode(
  {
    for(i in 1:n) {
      y[i] ~ dgamma(shape = beta[i], scale = lambda[i])
      beta[i] <- betaStar[z[i]]
      lambda[i] <- lambdaStar[z[i]]
      z[i] ~ dcat(w[1:Trunc])
    }
    for(i in 1:(Trunc-1)) { # stick breking variables
      v[i] ~ dbeta(1, alpha)
    }
    w[1:Trunc] <- stick_breaking(v[1:(Trunc-1)]) # stick breaking weights
    for(i in 1:Trunc) {
      betaStar[i] ~ dgamma(shape = 71, scale = 2)
      lambdaStar[i] ~ dgamma(shape = 2, scale = 2)
    }
    alpha ~ dgamma(1, 1)
  }
)
```
Note that the truncation level $L$ of $G$ has been set to a value 'Trunc', which is to be defined in the 'constants' argument of the nimbleModel function.


#### Running the MCMC algorithm
The following code sets up the model data and constants, initializes the parameters, defines the model object, and builds and runs the MCMC algorithm for the mixture of Gamma distributions.

```{r, MCMC-gammadensity-bnp-SB}
data <- list(y = faithful$waiting)
set.seed(1)
consts <- list(n = length(faithful$waiting), Trunc = 50)
inits <- list(betaStar = rgamma(consts$Trunc, shape = 71, scale = 2),
              lambdaStar = rgamma(consts$Trunc, shape = 2, scale = 2),
              v = rbeta(consts$Trunc-1, 1, 1),
              z = sample(1:10, size = consts$n, replace = TRUE), 
              alpha = 1)

rmodel <- nimbleModel(code, data = data, inits = inits, constants = consts) 
cmodel <- compileNimble(rmodel) 
mConf <- configureMCMC(rmodel, monitors = c("w", "betaStar", "lambdaStar", 'z', 'alpha')) 
mMCMC <- buildMCMC(mConf) 
cMCMC <- compileNimble(mMCMC, project = rmodel)  
samples <- runMCMC(cMCMC, niter = 2000, nburnin = 1000, setSeed = TRUE)
```

Using the stick-breaking approximation automatically provides an approximation $G_L$ of the random distribution $G$.  The following code computes posterior samples of $G_L$ using posterior samples from the 'samples' object, and from them, a density estimate for the data.

```{r, MCMC-gammadensity-bnp-SB-G}
betaStarSamples <- samples[ , grep('betaStar', colnames(samples))]
lambdaStarSamples <- samples[ , grep('lambdaStar', colnames(samples))]
wSamples <- samples[ , grep('w', colnames(samples))]

grid <- seq(40, 100, len=200)
Trunc <- consts$Trunc

postPredicitveSamples <- matrix(0, ncol = length(grid), nrow = nrow(samples))
for(i in 1:nrow(samples)) {
  postPredicitveSamples[i, ] <- sapply(grid, function(x)sum(wSamples[i, ]*dgamma(x, 
                                                shape = betaStarSamples[i, ],
                                                scale = lambdaStarSamples[i, ])))
}


# plot:
hist(faithful$waiting, freq=FALSE,  main="Old Faithful data") # histogram of original data
lines(grid, apply(postPredicitveSamples, 2, mean), lwd=2, col='black') 
lines(grid, apply(postPredicitveSamples, 2, mean), lwd=2, col='black') 
lines(grid, apply(postPredicitveSamples, 2, emp.hpd)[1, ], lwd=2, lty=2, col='grey')
lines(grid, apply(postPredicitveSamples, 2, emp.hpd)[2, ], lwd=2, lty=2, col='grey')
```

As expected, this estimate looks identical to the one we obtained through the CRP representation of the process.

## Nonparametric random effect distributions for Generalized Linear Mixed Models

We will illustrate the use of nonparametric mixture models for modeling random effects distributions in the context of a meta-analysis of the side effects of a formerly very popular drug for diabetes called Avandia. The data we analyze played a role in raising serious questions about the safety of this drug.  The question is whether Avandia use increases the risk of myocardial infarction (heart attack). There are 48 studies (the 49th study in the data file is different in some ways and excluded here), each with treatment and control arms.
```{r, avandia-view}
dat <- read.csv('https://rawgit.com/nimble-dev/nimble-demos/master/intro_bnp/avandia.csv')
head(dat)
```


### Basic meta analysis of Avandia myocardial infarctions (MIs)

#### Model formulation

We begin with a standard generalized linear mixed model (GLMM)-based meta analysis. The vectors $n$ and $x$ contain the total number of patients in the control and the number of patients suffering from myocardial infarctions in the control group of each study, respectively.  Similarly, the vectors $m$ and $y$ contain similar information for patients receiving the drug Avandia.  The model takes the form


$$
x_{i} \mid \theta, \gamma_i \sim \mbox{Bin} \left(n_i,   \frac{1}{1 + \exp\left\{ -\gamma_i  \right\}} \right) , \quad\quad y_{i} \mid \theta, \gamma_i \sim \mbox{Bin} \left(m_i,   \frac{1}{1 + \exp\left\{ -(\theta + \gamma_i)  \right\}} \right) 
$$
where the random effects $\gamma_i$ follow a common normal distributions, $\gamma_i \sim \mbox{N}(0, \tau^2)$, and the $\theta$ and $\tau^2$ are given reasonably non-informative priors.  The parameter $\theta$ quantifies the difference in risk between the control and treatment arms, while the $\gamma_i$ quantify study-specific variation.

This model can be specified in NIMBLE using the following code:
```{r, avandia-setup}
dat <- read.csv('avandia.csv')
dat <- dat[-49, ]

x <- dat$controlMI
n <- dat$nControl
y <- dat$avandiaMI
m <- dat$nAvandia

nStudies <- nrow(dat)
data <- list(x = x, y = y)
constants = list(n = n, m = m, nStudies = nStudies)

codeParam <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu, sd = tau)        # study effects
    }
    theta ~ dflat()        # effect of Avandia
    # random effects hyperparameters
    mu ~ dflat()
    tau ~ dunif(0, 100)
})
```


#### Running the MCMC

Let's run a basic MCMC.

```{r, mcmc, fig.cap='', fig.width=12, fig.height=5}
inits = list(theta = 0, mu = 0, tau = 1, gamma = rnorm(nStudies))

samples <- nimbleMCMC(code = codeParam, data = data, inits = inits,
                      constants = constants, monitors = c("mu", "tau", "theta", "gamma"),
                      thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)
gammaCols <- grep('gamma', colnames(samples))

par(mfrow = c(1, 4))
ts.plot(samples[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samples[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samples[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects', main = 'random effects distribution')
hist(samples[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')
```

The results suggests there is an overall difference in risk between the control and treatment arms.  But what about the normality assumption? Are our conclusions robust to that assumption? Perhaps the random effects distribution are skewed. (And recall that the estimates above of the random effects are generated under the normality assumption, which pushes the estimated effects to look more normal...)


### DP-based random effects modeling for meta analysis

#### Model formulation

Now, we use a nonparametric distribution for $\gamma_i$s.  More specifically, we assume that each $\gamma_i$ is generated from a location-scale mixture of normal distributions:
$$
\gamma_i \mid \mu_i, \tau_i^2 \sim \mbox{N}(\mu_i, \tau_i^2),   \quad\quad  (\mu_i, \tau_i^2) \mid G \sim G,   \quad\quad     G \sim \mbox{DP}(\alpha, H),
$$
where $H$ is again a normal-inverse-gamma distribution.

This specification induces clustering among the random effect.  As in the case of density estimation problems, the DP prior allows the data to determine the number of components, from as few as one component (i.e., simplifying to the parametric model), to as many as $n$ components, i.e., one component for each observation. This allows the distribution of the random effects to be multimodal if the data supports such behavior, greatly increasing its flexibility.  This model can be specified in NIMBLE using the following code:
```{r, meta-bnp}
codeBNP <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu[i], var = tau[i])  # random effects from mixture dist.
        mu[i] <- muTilde[xi[i]]               # mean for random effect from cluster xi[i]
        tau[i] <- tauTilde[xi[i]]             # var for random effect from cluster xi[i]
    }
    # mixture component parameters drawn from base measures
    for(i in 1:nStudies) {
        muTilde[i] ~ dnorm(mu0, sd = sd0)
        tauTilde[i] ~ dinvgamma(a0, b0)
    }
    # CRP for clustering studies to mixture components
    xi[1:nStudies] ~ dCRP(alpha, size = nStudies)
    # hyperparameters
    alpha ~ dgamma(1, 1)      
    mu0 ~ dflat()
    sd0 ~ dunif(0, 100)
    a0 ~ dunif(0, 100)
    b0 ~ dunif(0, 100)
    theta ~ dflat()          # effect of Avandia
})
```


### Running an MCMC for the DP-based meta analysis

The following code compiles the model and runs a collapsed Gibbs sampler for the model
```{r, DP-MCMC, fig.cap='', fig.width=12, fig.height=5}
inits <- list(gamma = rnorm(nStudies), xi = sample(1:2, nStudies, replace = TRUE),
              alpha = 1, mu0 = 0, sd0 = 1, a0 = 1, b0 = 1, theta = 0,
              muTilde = rnorm(nStudies), tauTilde = rep(1, nStudies))

samplesBNP <- nimbleMCMC(code = codeBNP, data = data, inits = inits,
               constants = constants,
               monitors = c("theta", "gamma", "alpha", "xi", "mu0", "sd0", "a0", "b0"),
               thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)

gammaCols <- grep('gamma', colnames(samplesBNP))
xiCols <- grep('xi', colnames(samplesBNP))

par(mfrow = c(1,5))
ts.plot(samplesBNP[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samplesBNP[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samplesBNP[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects',
              main = 'random effects distribution')
hist(samplesBNP[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')

# How many mixture components are inferred?
xiRes <- samplesBNP[ , xiCols]
nGrps <- apply(xiRes, 1, function(x) length(unique(x)))
ts.plot(nGrps, xlab = 'iteration', ylab = 'number of components')
```

The primary inference seems robust.  This is probably driven by the fact that there is not much evidence of lack of normality in the random effects distribution (as evidenced by the fact that the posterior distribution of the number of mixture components places a large amount of probabilty on exactly one component).

## More information and future development

Please see our [User Manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html)

We're in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our [announcement](https://groups.google.com/forum/#!forum/nimble-announce) or [user support/discussion](https://groups.google.com/forum/#!forum/nimble-users) Google groups. 
















