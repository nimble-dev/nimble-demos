---
title: "Bayesian Nonparametric Models in NIMBLE"
subtitle: "September 2018"
author: "Claudia Wehrhahn, Chris Paciorek, and the NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE} 
library(methods) ## needed only when building documents outside of R
library(nimble)
```

NIMBLE is a hierarchical modeling package that uses nearly the same modeling language as the popular MCMC packages WinBUGS, OpenBUGS and JAGS, while making the modeling language extensible -- you can add distributions and functions -- and also allows customization of MCMC or other algorithms that use models.

Recently we added support Bayesian nonparametric (BNP) mixture modeling to NIMBLE. In particular, NIMBLE provides Dirichlet process mixture models using either the Chinese Restaurant Process or stick-breaking representation. 

We'll illustrate NIMBLE's BNP capabilities by taking a parametric random effects model and easily switching to a nonparametric representation of the random effects that avoids the assumption of normally-distributed random effects.

Then we'll show how one can do nonparametric density estimation.

# Nonparametric random effects

We'll illustrate using a meta-analysis of the side effects of a formerly very popular drug for diabetes called Avandia. We'll analyze data that played a role in raising serious questions about the safety of Avandia. The question is whether Avandia use increases the risk of myocardial infarction (heart attack). There are 48 studies (the 49th study in the data file is different in some ways and excluded here), each with treatment and control arms.

```{r, avandia-view}
dat <- read.csv('avandia.csv')
head(dat)
```

We'll start with a standard generalized linear mixed model (GLMM)-based meta analysis. 

# Basic meta analysis of Avandia myocardial infarctions (MIs)

```{r, avandia-setup}
dat <- read.csv('avandia.csv')
dat <- dat[-49, ]

x <- dat$controlMI
n <- dat$nControl
y <- dat$avandiaMI
m <- dat$nAvandia

nStudies <- nrow(dat)
data <- list(x = x, y = y)
constants = list(n = n, m = m, nStudies = nStudies)

codeParam <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu, sd = tau)        # study effects
    }
    theta ~ dflat()        # effect of Avandia
    # random effects hyperparameters
    mu ~ dflat()
    tau ~ dunif(0, 100)
})
```

$\theta$ quantifies the difference in risk between the control and treatment arms, while the $\gamma_i$ quantify study-specific variation using normally-distributed random effects.

## Running the MCMC

Let's run a basic MCMC.

```{r, mcmc, fig.cap='', fig.width=12, fig.height=5}
inits = list(theta = 0, mu = 0, tau = 1, gamma = rnorm(nStudies))

samples <- nimbleMCMC(code = codeParam, data = data, inits = inits,
                      constants = constants, monitors = c("mu", "tau", "theta", "gamma"),
                      thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)
gammaCols <- grep('gamma', colnames(samples))

par(mfrow = c(1, 4))
ts.plot(samples[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samples[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samples[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects', main = 'random effects distribution')
hist(samples[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')
```

This suggests there is an overall difference in risk between the control and treatment arms.

But what about the normality assumption? Are our conclusions robust to that assumption? Perhaps the random effects distribution are skewed. (And recall that the estimates above of the random effects are generated under the normality assumption, which pushes the estimated effects to look more normal...)


## DP-based random effects modeling for meta analysis

Here we allow each $\gamma_i$ to be clustered with a set of the other random effects.
Each $\gamma_i$ is generated from a normal distribution associated with a specific mixture component,
where the mean and variance of the normal distribution are specific to the mixture component.
This induces clustering, with the random effects grouped into clusters that share the same parameters.
The DP specification allows the data to determine the number of components, from as few as one
component (i.e., simplifying to the parametric model) to as many as $n$ components, i.e., one component
for each observation, allowing extreme flexibility. 

```{r, meta-bnp}
codeBNP <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu[i], var = tauTilde[i])  # random effects from mixture dist.
        mu[i] <- muTilde[xi[i]]               # mean for random effect from cluster xi[i]
        tau[i] <- tauTilde[xi[i]]             # var for random effect from cluster xi[i]
    }
    # mixture component parameters drawn from base measures
    for(i in 1:nStudies) {
        muTilde[i] ~ dnorm(mu0, sd = sd0)
        tauTilde[i] ~ dinvgamma(a0, b0)
    }
    # CRP for clustering studies to mixture components
    xi[1:nStudies] ~ dCRP(alpha, size = nStudies)
    # hyperparameters
    alpha ~ dgamma(1, 1)      
    mu0 ~ dflat()
    sd0 ~ dunif(0, 100)
    a0 ~ dunif(0, 100)
    b0 ~ dunif(0, 100)
    theta ~ dflat()          # effect of Avandia
})
```

The specification is a bit complicated, but just think of it as using a nonparametric extension to a mixture of normal distributions as the random effects distribution for $\gamma_i$, but where we don't fix the maximum number of components.

## Running an MCMC for the DP-based meta analysis

```{r, DP-MCMC, fig.cap='', fig.width=12, fig.height=5}
inits <- list(gamma = rnorm(nStudies), xi = sample(1:2, nStudies, replace = TRUE),
              alpha = 1, mu0 = 0, sd0 = 1, a0 = 1, b0 = 1, theta = 0,
              muTilde = rnorm(nStudies), tauTilde = rep(1, nStudies))

samplesBNP <- nimbleMCMC(code = codeBNP, data = data, inits = inits,
               constants = constants,
               monitors = c("theta", "gamma", "alpha", "xi", "mu0", "sd0", "a0", "b0"),
               thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)

gammaCols <- grep('gamma', colnames(samplesBNP))
xiCols <- grep('xi', colnames(samplesBNP))

par(mfrow = c(1,5))
ts.plot(samplesBNP[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samplesBNP[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samplesBNP[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects',
              main = 'random effects distribution')
hist(samplesBNP[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')

# How many mixture components are inferred?
xiRes <- samplesBNP[ , xiCols]
nGrps <- apply(xiRes, 1, function(x) length(unique(x)))
ts.plot(nGrps, xlab = 'iteration', ylab = 'number of components')
```

Conclusions: the primary inference seems robust, and there's also not much evidence of multiple components.


# Using other kernels

NIMBLE allows you to easily use any kernel of your choice. For example we can replace the mixture of normals in the Avandia example with a mixture of t distributions. [[[do we want to actually show this in the example? use a different example so we can show a kernel such as a gamma or beta?]]]

# Basic density estimation

[[[Did we decide on an example for this? The geyser dataset from R?]]]

# More information and future development

Please see our [User Manual](https://r-nimble.org/html_manual/cha-welcome-nimble.html)

We're in the midst of improvements to the existing BNP functionality as well as adding additional Bayesian nonparametric models, such as hierarchical Dirichlet processes and Pitman-Yor processes, so please add yourself to our [announcement](https://groups.google.com/forum/#!forum/nimble-announce) or [user support/discussion](https://groups.google.com/forum/#!forum/nimble-users) Google groups. 
